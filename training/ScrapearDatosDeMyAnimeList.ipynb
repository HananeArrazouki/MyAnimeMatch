{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKbKf28wGHUI"
      },
      "source": [
        "La pagina que vamos a scrapear es: [MyAnimeList](https://myanimelist.net/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6ey9YtZIeWa"
      },
      "source": [
        "## Librerias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6EKaqTo4Z_i"
      },
      "outputs": [],
      "source": [
        "!pip install bs4\n",
        "!pip install requests\n",
        "!pip install lxml\n",
        "!pip install boto3\n",
        "!pip install awscli"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2kKjkF5I0YR"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup, Tag\n",
        "import time\n",
        "import os\n",
        "import io\n",
        "import pandas as pd\n",
        "import bs4\n",
        "import re\n",
        "import csv, os, glob, boto3, requests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Q-DGMkZJILg"
      },
      "source": [
        "## Obtención de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDopbMRAWMcX"
      },
      "outputs": [],
      "source": [
        "def extract_ranked_number(html_fragment):\n",
        "    # Usamos una expresión regular para encontrar el número dentro del fragmento HTML\n",
        "    match = re.search(r'#(\\d+)', html_fragment)\n",
        "    if match:\n",
        "        return match.group(1)  # Devuelve el primer grupo capturado, que es el número de clasificación\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Función para obtener datos de una página de anime específica\n",
        "def scrape_anime_info(anime_id):\n",
        "    url = f\"https://myanimelist.net/anime/{anime_id}\"\n",
        "    response = requests.get(url)\n",
        "    if response.status_code != 200:\n",
        "        return None\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Buscar título\n",
        "    # title = soup.find('h1', class_='title-name').get_text(strip=True) if soup.find('h1', class_='title-name') else 'No title found'\n",
        "    try:\n",
        "        title = soup.find('h1', class_='title-name').get_text(strip=True)\n",
        "    except AttributeError:\n",
        "        # Si no se encuentra el título, imprime un mensaje y devuelve None\n",
        "        print(f'No title found for ID {anime_id}')\n",
        "        return None\n",
        "\n",
        "    img_meta = soup.find('meta', property='og:image')\n",
        "    image_url = img_meta['content'] if img_meta else None\n",
        "\n",
        "    if image_url:\n",
        "        anime_id_from_url = image_url.split('/')[-2]\n",
        "    else:\n",
        "        anime_id_from_url = None\n",
        "\n",
        "    # Definir la URL de imagen de error si no se encuentra ninguna imagen\n",
        "    error_image_url = \"https://cdn.myanimelist.net/images/error/404_image_transparent.png?v=170126\"\n",
        "\n",
        "    # Diccionario para almacenar los datos\n",
        "    data = {\n",
        "        'ID': anime_id,\n",
        "        'Title': title,\n",
        "        'Type': None,\n",
        "        'Episodes': None,\n",
        "        'Source': None,\n",
        "        'Studios': None,\n",
        "        'Demographic': None,\n",
        "        'Year': None,\n",
        "        'Producers': None,\n",
        "        'Genres': None,\n",
        "        'Licensors': None,\n",
        "        'Duration': None,\n",
        "        'Rating': None,\n",
        "        'Score': None,\n",
        "        'Ranked': None,\n",
        "        'Popularity': None,\n",
        "        'Members': None,\n",
        "        'Favorites': None,\n",
        "        'Image_URL': image_url if image_url else error_image_url,\n",
        "        'Synopsis': None\n",
        "    }\n",
        "\n",
        "    # Buscar los divs con la clase 'spaceit_pad'\n",
        "    for div in soup.find_all(\"div\", class_=\"spaceit_pad\"):\n",
        "        span = div.find(\"span\", class_=\"dark_text\")\n",
        "        if span:\n",
        "            key = span.text.strip().rstrip(':')\n",
        "            value = ''.join([str(s) if isinstance(s, bs4.element.Tag) else s for s in span.next_siblings]).strip()\n",
        "            if 'Type' in key:\n",
        "                type_tag = div.find(\"a\")\n",
        "                if type_tag:\n",
        "                    data['Type'] = type_tag.get_text()\n",
        "            elif 'Episodes' in key:\n",
        "                data['Episodes'] = int(value.split()[0]) if value.split()[0] != 'Unknown' else None\n",
        "            elif 'Source' in key:\n",
        "                data['Source'] = value\n",
        "            elif 'Studios' in key:\n",
        "                data['Studios'] = ', '.join([a.text for a in div.find_all(\"a\")])\n",
        "            elif 'Demographic' in key:\n",
        "                demographic_tag = div.find(\"a\")\n",
        "                if demographic_tag:\n",
        "                    data['Demographic'] = demographic_tag.get_text()\n",
        "            elif 'Aired' in key:\n",
        "                aired_year = value.split(',')[1].split(' ')[1]\n",
        "                data['Year'] = int(aired_year) if aired_year.isdigit() else None\n",
        "            elif 'Producers' in key:\n",
        "                data['Producers'] = ', '.join([a.text for a in div.find_all(\"a\")])\n",
        "            elif ('Genre' in key) or ('Genres' in key):\n",
        "                data['Genres'] = ', '.join([a.text for a in div.find_all(\"a\")])\n",
        "            elif 'Licensors' in key:\n",
        "                data['Licensors'] = None if 'None' in value else ', '.join([a.text for a in div.find_all(\"a\")])\n",
        "            elif 'Duration' in key:\n",
        "                data['Duration'] = int(value.split(' ')[0])\n",
        "            elif 'Rating' in key:\n",
        "                data['Rating'] = value\n",
        "            elif 'Score' in key:\n",
        "                score_match = re.search(r'(\\d+(\\.\\d+)?)', value)\n",
        "                if score_match:\n",
        "                    data['Score'] = float(score_match.group(1))\n",
        "                else:\n",
        "                    data['Score'] = None\n",
        "            elif 'Ranked' in key:\n",
        "                ranked_number = extract_ranked_number(value)\n",
        "                data['Ranked'] = ranked_number\n",
        "            elif 'Popularity' in key:\n",
        "                popularity_number = extract_ranked_number(value)\n",
        "                data['Popularity'] = popularity_number\n",
        "            elif 'Members' in key:\n",
        "                data['Members'] = value\n",
        "            elif 'Favorites' in key:\n",
        "                data['Favorites'] = value\n",
        "\n",
        "\n",
        "    # Extraer Synopsis\n",
        "    synopsis_div = soup.find('p', itemprop=\"description\")\n",
        "    if synopsis_div:\n",
        "        data['Synopsis'] = synopsis_div.get_text(strip=True)\n",
        "\n",
        "    return data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrG9lQqSz-1-"
      },
      "outputs": [],
      "source": [
        "def extract_ranked_number(html_fragment):\n",
        "    # Expresión regular para encontrar el número dentro del fragmento HTML\n",
        "    match = re.search(r'#(\\d+)', html_fragment)\n",
        "    if match:\n",
        "        return match.group(1)\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Función para obtener datos de una página de anime específica\n",
        "def scrape_anime_info(anime_id):\n",
        "    url = f\"https://myanimelist.net/anime/{anime_id}\"\n",
        "    response = requests.get(url)\n",
        "    if response.status_code != 200:\n",
        "        return None\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    try:\n",
        "        # Buscar el título\n",
        "        title = soup.find('h1', class_='title-name').get_text(strip=True)\n",
        "    except AttributeError:\n",
        "        # Si no se encuentra el título, imprime un mensaje y devuelve None\n",
        "        print(f'No title found for ID {anime_id}')\n",
        "        return None\n",
        "\n",
        "    img_meta = soup.find('meta', property='og:image')\n",
        "    image_url = img_meta['content'] if img_meta else None\n",
        "\n",
        "    if image_url:\n",
        "        anime_id_from_url = image_url.split('/')[-2]\n",
        "    else:\n",
        "        anime_id_from_url = None\n",
        "\n",
        "    # Definir la URL de imagen de error si no se encuentra ninguna imagen\n",
        "    error_image_url = \"https://cdn.myanimelist.net/images/error/404_image_transparent.png\"\n",
        "\n",
        "    # Diccionario para almacenar los datos\n",
        "    data = {\n",
        "        'ID': anime_id,\n",
        "        'Title': title,\n",
        "        'Type': None,\n",
        "        'Episodes': None,\n",
        "        'Source': None,\n",
        "        'Studios': None,\n",
        "        'Demographic': None,\n",
        "        'Year': None,\n",
        "        'Producers': None,\n",
        "        'Genres': None,\n",
        "        'Licensors': None,\n",
        "        'Duration': None,\n",
        "        'Rating': None,\n",
        "        'Score': None,\n",
        "        'Ranked': None,\n",
        "        'Popularity': None,\n",
        "        'Members': None,\n",
        "        'Favorites': None,\n",
        "        'Image_URL': image_url if image_url else error_image_url,\n",
        "        'Synopsis': None\n",
        "    }\n",
        "\n",
        "    # Buscar los divs con la clase 'spaceit_pad'\n",
        "    for div in soup.find_all(\"div\", class_=\"spaceit_pad\"):\n",
        "        span = div.find(\"span\", class_=\"dark_text\")\n",
        "        if span:\n",
        "            key = span.text.strip().rstrip(':')\n",
        "            value = ''.join([str(s) if isinstance(s, bs4.element.Tag) else s for s in span.next_siblings]).strip()\n",
        "            if 'Type' in key:\n",
        "                type_tag = div.find(\"a\")\n",
        "                if type_tag:\n",
        "                    data['Type'] = type_tag.get_text()\n",
        "            elif 'Episodes' in key:\n",
        "                data['Episodes'] = int(value.split()[0]) if value.split()[0] != 'Unknown' else None\n",
        "            elif 'Source' in key:\n",
        "                data['Source'] = value\n",
        "            elif 'Studios' in key:\n",
        "                data['Studios'] = ', '.join([a.text for a in div.find_all(\"a\")])\n",
        "            elif 'Demographic' in key:\n",
        "                demographic_tag = div.find(\"a\")\n",
        "                if demographic_tag:\n",
        "                    data['Demographic'] = demographic_tag.get_text()\n",
        "            elif 'Aired' in key:\n",
        "                aired_year = value.split(',')[1].split(' ')[1]\n",
        "                data['Year'] = int(aired_year) if aired_year.isdigit() else None\n",
        "            elif 'Producers' in key:\n",
        "                data['Producers'] = ', '.join([a.text for a in div.find_all(\"a\")])\n",
        "            elif ('Genre' in key) or ('Genres' in key):\n",
        "                data['Genres'] = ', '.join([a.text for a in div.find_all(\"a\")])\n",
        "            elif 'Licensors' in key:\n",
        "                data['Licensors'] = None if 'None' in value else ', '.join([a.text for a in div.find_all(\"a\")])\n",
        "            elif 'Duration' in key:\n",
        "                data['Duration'] = int(value.split(' ')[0])\n",
        "            elif 'Rating' in key:\n",
        "                data['Rating'] = value\n",
        "            elif 'Score' in key:\n",
        "                score_match = re.search(r'(\\d+(\\.\\d+)?)', value)\n",
        "                if score_match:\n",
        "                    data['Score'] = float(score_match.group(1))\n",
        "                else:\n",
        "                    data['Score'] = None\n",
        "            elif 'Ranked' in key:\n",
        "                ranked_number = extract_ranked_number(value)\n",
        "                data['Ranked'] = ranked_number\n",
        "            elif 'Popularity' in key:\n",
        "                popularity_number = extract_ranked_number(value)\n",
        "                data['Popularity'] = popularity_number\n",
        "            elif 'Members' in key:\n",
        "                data['Members'] = value\n",
        "            elif 'Favorites' in key:\n",
        "                data['Favorites'] = value\n",
        "\n",
        "\n",
        "    # Extraer Synopsis\n",
        "    synopsis_div = soup.find('p', itemprop=\"description\")\n",
        "    if synopsis_div:\n",
        "        data['Synopsis'] = synopsis_div.get_text(strip=True)\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importa el módulo traceback para imprimir información detallada sobre las excepciones\n",
        "import traceback\n",
        "\n",
        "# Abrir archivo CSV para escribir\n",
        "with open('scraping_animeList.csv', 'w', newline='', encoding='utf-8') as file:\n",
        "    fieldnames = ['ID', 'Title', 'Type', 'Episodes', 'Source', 'Studios', 'Demographic', 'Year', 'Producers', 'Genres', 'Licensors',\n",
        "                  'Duration', 'Rating', 'Score', 'Ranked', 'Popularity', 'Members', 'Favorites', 'Image_URL', 'Synopsis']\n",
        "    writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
        "    writer.writeheader()\n",
        "\n",
        "    # Número máximo de intentos para cada solicitud\n",
        "    MAX_ATTEMPTS = 3\n",
        "\n",
        "    # Tiempo de espera entre intentos en segundos\n",
        "    WAIT_TIME = 5\n",
        "\n",
        "    # Iterar sobre un rango de IDs de anime\n",
        "    # Aquí había intentado con este rango y con otro más pequeño, guardando los resultados en listas para luego poder juntarlos en un solo dataset.\n",
        "    # Sin embargo, no he podido lograrlo porque algo extraño sucedía. Parecía perderse la conexión o algo similar, ya que no se guardaban los resultados\n",
        "    # correctamente después de las primeras 100 páginas. Es decir, las primeras 100 páginas se guardaban correctamente, tanto las que tenían resultados\n",
        "    # como aquellas que mostraban el mensaje de que no había resultados. Pero luego de esas 100 páginas, el proceso empezaba a fallar y devolvía el mensaje\n",
        "    # de que la página no existía. Esto me llevó a tener que hacerlo manualmente, guardando cada 100 páginas en un archivo diferente y luego uniendo todos\n",
        "    # los archivos al final para formar un solo dataset.\n",
        "    for anime_id in range(1, 60000):\n",
        "        attempts = 0\n",
        "        while attempts < MAX_ATTEMPTS:\n",
        "            try:\n",
        "                anime_data = scrape_anime_info(anime_id)\n",
        "                if anime_data:\n",
        "                    writer.writerow(anime_data)\n",
        "                    break\n",
        "                else:\n",
        "                    print(f'No data found for ID {anime_id}')\n",
        "                    break\n",
        "            except Exception as e:\n",
        "                print(f'Error occurred for ID {anime_id}: {e}')\n",
        "                print(traceback.format_exc())\n",
        "                attempts += 1\n",
        "                time.sleep(WAIT_TIME)\n",
        "\n",
        "        if attempts == MAX_ATTEMPTS:\n",
        "            print(f'Failed to scrape data for ID {anime_id} after {MAX_ATTEMPTS} attempts')\n",
        "\n",
        "        # Pausa para evitar sobrecargar el servidor\n",
        "        time.sleep(1)\n",
        "\n",
        "print(\"Scraping completo.\")"
      ],
      "metadata": {
        "id": "cAPeIR3PlC7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEoiDYmAx_Wv"
      },
      "source": [
        "## Almacenar en S3"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aquí dejo estas funciones que intenté utilizar para guardar el resultado en un bucket de S3, pero no he logrado hacerlo. Siempre me daba errores de credenciales. He preguntado a mis compañeros y lo habían hecho de la misma manera, pero a mí no me ha funcionado. También lo hemos hecho de la misma manera en el proyecto final de Big Data con credenciales de una cuenta profesional, y funcionó. Sin embargo, no sé por qué en mi caso no funcionó; no pude encontrar el problema. Lo dejo aquí para intentarlo en el futuro, ya que no he tenido tiempo para investigar más."
      ],
      "metadata": {
        "id": "GDHFTsZMm5M4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0dWSiOUMKyf"
      },
      "outputs": [],
      "source": [
        "# def upload_to_s3(local_file, bucket, s3_file=None):\n",
        "#   # Credenciales de AWS\n",
        "#   s3 = boto3.client('s3', aws_access_key_id=\"ASIAYM5Z2YZLGCW5XYEX\", aws_secret_access_key=\"4CVY3YoRnzSSVt38VdRKA/z6zfW/agmilHsAxzYV\")\n",
        "\n",
        "#   if s3_file is None:\n",
        "#       s3_file = os.path.basename(local_file)\n",
        "\n",
        "#   s3.upload_file(local_file, bucket, s3_file)\n",
        "#   print(f'{local_file} ha sido subido a {bucket}/{s3_file}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Psjek7RuyVit"
      },
      "outputs": [],
      "source": [
        "# bucket = \"scrapingwebiabd\"\n",
        "# files = glob.glob(\"/content/drive/MyDrive/dataset/tfm/scraping_animeList.csv\")\n",
        "\n",
        "# for file in files:\n",
        "#     upload_to_s3(file, bucket)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def upload_to_s3(local_file, bucket, s3_file=None):\n",
        "#     try:\n",
        "#         # Usar las credenciales configuradas en el entorno/ perfil de AWS\n",
        "#         s3 = boto3.client('s3', region_name='us-east-1')\n",
        "#         if s3_file is None:\n",
        "#             s3_file = os.path.basename(local_file)\n",
        "\n",
        "#         s3.upload_file(local_file, bucket, s3_file)\n",
        "#         print(f'{local_file} ha sido subido exitosamente a {bucket}/{s3_file}')\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error al subir: {str(e)}\")\n",
        "\n",
        "# bucket = \"scrapingwebiabd\"\n",
        "# files = glob.glob(\"/content/scraping_animeList.csv\")\n",
        "\n",
        "# for file in files:\n",
        "#     upload_to_s3(file, bucket)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAXONo96s54l",
        "outputId": "63d144fd-c479-4c8e-e1d3-0c9ed5327897"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error al subir: Unable to locate credentials\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49w0EKctE8LX"
      },
      "outputs": [],
      "source": [
        "# Guardamos el DataFrame en un archivo CSV en la ruta especificada\n",
        "# dataframes = [df]\n",
        "# fusion = pd.concat(dataframes)\n",
        "# ruta_csv = \"/content/all_anime_data.csv\"\n",
        "# fusion.to_csv(ruta_csv, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHXECXQkFuWE"
      },
      "outputs": [],
      "source": [
        "# def upload_to_s3(local_file, bucket, s3_file=None):\n",
        "#     # Credenciales de AWS\n",
        "#     s3 = boto3.client('s3', aws_access_key_id=\"ASIAYM5Z2YZLHJG22QFI\", aws_secret_access_key=\"hOkCvnp4P0A5Obd98qkt945b5LUF5FVEI4cwIUDz\")\n",
        "\n",
        "#     if s3_file is None:\n",
        "#         s3_file = os.path.basename(local_file)\n",
        "\n",
        "#     s3.upload_file(local_file, bucket, s3_file) # Si no hay errores, se suben los archivos a S3\n",
        "#     print(f'{local_file} ha sido subido exitosamente a {bucket}/{s3_file}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AzJbY-KyGCyI"
      },
      "outputs": [],
      "source": [
        "# # Subimos los archivos al bucket llamando a la función creada\n",
        "# bucket = \"scrapingwebiabd\"\n",
        "# files = glob.glob(\"/content/all_anime_data.csv\")\n",
        "\n",
        "# for file in files:\n",
        "#     upload_to_s3(file, bucket)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}